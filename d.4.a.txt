
**************** HPC 6 CUDA VECTOR ADDITION ********************************************

#include <iostream>  
#include <cuda_runtime.h>
#include /usr/local/cuda/include/cuda_runtime.h

_global_void addVectors(int* A, int* B, int* C, int n)
{
	int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i < n)
	{
    	C[i] = A[i] + B[i];
	}
}

int main()
{
	int n = 1000000;  
	int* A, * B, * C;
	int size = n * sizeof(int);

	// Allocate memory on the host  
	cudaMallocHost(&A, size);  
	cudaMallocHost(&B, size);  
	cudaMallocHost(&C, size);

	// Initialize the vectors
	for (int i = 0; i < n; i++)
	{
    	A[i] = i;
    	B[i] = i * 2;
	}
	// Allocate memory on the device  
	int* dev_A, * dev_B, * dev_C;  
	cudaMalloc(&dev_A, size);  
	cudaMalloc(&dev_B, size);  
	cudaMalloc(&dev_C, size);

	// Copy data from host to device
	cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);  
	cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);

	// Launch the kernel  
	int blockSize = 256;
	int numBlocks = (n + blockSize - 1) / blockSize;
	// Copy data from device to host
	cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);

	// Print the results
	for (int i = 0; i < 10; i++)
	{
    	cout << C[i] << " ";
	}
	cout << endl;

	// Free memory  
	cudaFree(dev_A);  
	cudaFree(dev_B);  
	cudaFree(dev_C);  
	cudaFreeHost(A);  
	cudaFreeHost(B);  
	cudaFreeHost(C);

	return 0;
}

CUDA (Compute Unified Device Architecture) is a platform and programming model created by NVIDIA for parallel computing. It enables developers to harness the computational capabilities of NVIDIA graphics processing units (GPUs) to accelerate various types of computations in applications such as scientific computing, machine learning, and computer vision.

With CUDA, developers have access to a collection of programming interfaces, libraries, and tools that facilitate the creation and execution of parallel code on NVIDIA GPUs. It supports widely-used programming languages like C, C++, and Python, and offers a simplified programming model that abstracts many of the intricate details of GPU architecture.

By leveraging CUDA, developers can take advantage of the immense parallelism and high computational power of GPUs to speed up demanding tasks like matrix operations, image processing, and deep learning. It opens up opportunities for faster and more efficient computation, enabling applications to achieve significant performance improvements.

Steps for Addition of two large vectors using CUDA:

Determine the size: Specify the size of the vectors that you want to add. This determines the number of threads and blocks needed for parallelization.

Allocate host memory: Allocate memory on the host (CPU) for the input vectors and the result vector using the C malloc function.

Initialize the vectors: Fill the input vectors with data on the host using a loop or other means.

Allocate device memory: Allocate memory on the device (GPU) for the input vectors and the result vector using the CUDA function cudaMalloc.

Copy input vectors to device: Transfer the input vectors from host memory to device memory using the CUDA function cudaMemcpy.

Launch the kernel: Launch a CUDA kernel, which is a GPU function, to perform the vector addition operation. The kernel will execute concurrently with multiple threads. Specify the number of blocks and threads to use using the <<<...>>> syntax.

Copy the result vector to host: Transfer the result vector from device memory to host memory using the CUDA function cudaMemcpy.

Free device memory: Release the memory that was allocated on the device using the CUDA function cudaFree.

Free host memory: Release the memory that was allocated on the host using the C free function.

By following these steps, you can utilize the parallel computing power of the GPU to perform efficient addition of large vectors using CUDA.
